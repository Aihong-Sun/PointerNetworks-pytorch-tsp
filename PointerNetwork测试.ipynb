{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def masked_log_softmax(vector:torch.Tensor, mask:torch.Tensor, dim:int=-1) -> torch.Tensor:\n",
    "    if mask is not None:\n",
    "        mask = mask.float()\n",
    "        while mask.dim() < vector.dim():\n",
    "            mask = mask.unsqueeze(1)\n",
    "        # vector + mask.log() is an easy way to zero out masked elements in log space, but it\n",
    "        # results in nans when the whole vector is masked.  We need a very small value instead of a\n",
    "        # zero in the mask for these cases.  log(1 + 1e-45) is still basically 0, so we can safely\n",
    "        # just add 1e-45 before calling mask.log().  We use 1e-45 because 1e-46 is so small it\n",
    "        # becomes 0 - this is just the smallest value we can actually use.\n",
    "        vector = vector + (mask + 1e-40).log()\n",
    "    return F.log_softmax(vector, dim=dim)\n",
    "\n",
    "def masked_max(vector:torch.Tensor, mask:torch.Tensor, dim:int, keep_dim:bool=False, min_val:float=-1e7) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    计算最大值：在masked值的特定的维度\n",
    "    :param vector: 计算最大值的vector，假定没有mask的部分全是0\n",
    "    :param mask: vector的mask，必须是可以扩展到vector的形状\n",
    "    :param dim: 计算max的维度\n",
    "    :param keep_dim: 是否保持dim\n",
    "    :param min_val: paddings的最小值\n",
    "    :return: 包括最大值的Tensor\n",
    "    \"\"\"\n",
    "    one_minus_mask = ~mask\n",
    "    replaced_vector = vector.masked_fill(one_minus_mask, min_val)\n",
    "    max_value, max_index = replaced_vector.max(dim=dim, keepdim=keep_dim)\n",
    "    return max_value, max_index\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, num_layers=1, batch_first=True, bidirectional=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_first = batch_first\n",
    "        self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=batch_first, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, embedded_inputs, input_lengths):\n",
    "        # 打包RNN的填充序列\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded_inputs, input_lengths.cpu(), batch_first=self.batch_first)\n",
    "        # forward pass through RNN\n",
    "        outputs, hidden = self.rnn(packed)\n",
    "        # Unpack padding\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=self.batch_first)\n",
    "        # 返回输出和最终状态\n",
    "        return outputs, hidden"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W1 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W2 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.vt = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, decoder_state, encoder_outputs, mask):\n",
    "        # (batch_size, max_seq_len, hidden_size)\n",
    "        encoder_transform = self.W1(encoder_outputs)\n",
    "\n",
    "        # (batch_size, 1(unsqueezed), hidden_size)\n",
    "        decoder_transform = self.W2(decoder_state).unsqueeze(1)\n",
    "\n",
    "        # (batch_size, max_seq_len, 1) => (batch_size, max_seq_len)\n",
    "        u_i = self.vt(torch.tanh(encoder_transform + decoder_transform)).squeeze(-1)\n",
    "\n",
    "        # softmax with only valid inputs, excluding zero padded parts\n",
    "        # log_softmax for a better numerical stability\n",
    "        log_score = masked_log_softmax(u_i, mask, dim=-1)\n",
    "        return log_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class PointerNet(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_size, bidirectional=True, batch_first=True):\n",
    "        super(PointerNet, self).__init__()\n",
    "        # Embedding dimension\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # decoder hidden size\n",
    "        self.hidden_size = hidden_size\n",
    "        # bidirectional encoder\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.num_layers = 1\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        # 我们将嵌入层用于以后更复杂的应用程序用法，例如单词序列。\n",
    "        self.embedding = nn.Linear(in_features=input_dim, out_features=embedding_dim,bias=False)\n",
    "        self.encoder = Encoder(embedding_dim=embedding_dim, hidden_size=hidden_size, num_layers=self.num_layers, bidirectional=bidirectional,batch_first=batch_first)\n",
    "\n",
    "        self.decoding_rnn = nn.LSTMCell(input_size=hidden_size, hidden_size=hidden_size)\n",
    "        self.attn = Attention(hidden_size=hidden_size)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths):\n",
    "        if self.batch_first:\n",
    "            batch_size = input_seq.size(0)\n",
    "            max_seq_len = input_seq.size(1)\n",
    "        else:\n",
    "            batch_size = input_seq.size(1)\n",
    "            max_seq_len = input_seq.size(0)\n",
    "\n",
    "        # embedding\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # (batch_size, max_seq_len, embedding_dim)\n",
    "\n",
    "        # encoder_output => (batch_size, max_seq_len, hidden_size) if batch_first else (max_seq_len, batch_size, hidden_size)\n",
    "        # hidden_size is usually set same as embedding size\n",
    "        # encoder_hidden => (num_layers * num_directions, batch_size, hidden_size) for each of h_n and c_n\n",
    "        encoder_outputs, encoder_hidden = self.encoder(embedded, input_lengths)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            # Optionally, Sum bidirectional RNN outputs\n",
    "            encoder_outputs = encoder_outputs[:, :, :self.hidden_size] + encoder_outputs[:, :, self.hidden_size:]\n",
    "\n",
    "        encoder_h_n, encoder_c_n = encoder_hidden\n",
    "        encoder_h_n = encoder_h_n.view(self.num_layers, self.num_directions, batch_size, self.hidden_size)\n",
    "        encoder_c_n = encoder_c_n.view(self.num_layers, self.num_directions, batch_size, self.hidden_size)\n",
    "\n",
    "        # Let's use zeros as an initial input\n",
    "        decoder_input = encoder_outputs.new_zeros(torch.Size((batch_size, self.hidden_size)))\n",
    "        decoder_hidden = (encoder_h_n[-1, 0, :, :].squeeze(), encoder_c_n[-1, 0, :, :].squeeze())\n",
    "\n",
    "        range_tensor = torch.arange(max_seq_len, device=input_lengths.device, dtype=input_lengths.dtype).expand(batch_size, max_seq_len, max_seq_len)\n",
    "        each_len_tensor = input_lengths.view(-1, 1, 1).expand(batch_size, max_seq_len, max_seq_len)\n",
    "\n",
    "        row_mask_tensor = (range_tensor < each_len_tensor)\n",
    "        col_mask_tensor = row_mask_tensor.transpose(1, 2)\n",
    "        mask_tensor = row_mask_tensor * col_mask_tensor\n",
    "\n",
    "        pointer_log_scores = []\n",
    "        pointer_argmaxs = []\n",
    "\n",
    "        for i in range(max_seq_len):\n",
    "            # we will simply mask out when calculating attention or max (and loss later)\n",
    "            # not all input and hidden, just for simplicity\n",
    "            sub_mask = mask_tensor[:, i, :]\n",
    "\n",
    "            # h,c:(batch_size, hidden_size)\n",
    "            h_i, c_i = self.decoding_rnn(decoder_input, decoder_hidden)\n",
    "\n",
    "            # next hidden\n",
    "            decoder_hidden = (h_i, c_i)\n",
    "\n",
    "            # get a pointer distribution over the encoder outputs using attention\n",
    "            # (batch_size, max_seq_len)\n",
    "            log_pointer_score = self.attn(h_i, encoder_outputs, sub_mask)\n",
    "            pointer_log_scores.append(log_pointer_score)\n",
    "\n",
    "            # get the indices of maximum pointer\n",
    "            # (batch_size, 1)\n",
    "            _, masked_argmax = masked_max(log_pointer_score, sub_mask, dim=1, keep_dim=True)\n",
    "\n",
    "            pointer_argmaxs.append(masked_argmax)\n",
    "\n",
    "            # (batch_size, 1, hidden_size)\n",
    "            index_tensor = masked_argmax.unsqueeze(-1).expand(batch_size, 1, self.hidden_size)\n",
    "\n",
    "            # encoder_outputs为(batch_size, max_seq_len, hidden_size)\n",
    "            # index为(batch_size, 1, hidden_size)，且所有hidden_size个的数据都是一样的，都是0-30的数字\n",
    "            # decoder_input: (batch_size , 1, hidden_size).squeeze(1)即(batch_size, hidden_size)\n",
    "            decoder_input = torch.gather(encoder_outputs, dim=1, index=index_tensor).squeeze(1)\n",
    "\n",
    "        # stack是叠加，会增加一个维度\n",
    "        # t * (batch_size, max_seq_len) t为max_seq_len, stack之后变成(batch_size, max_seq_len, max_seq_len)\n",
    "        pointer_log_scores = torch.stack(pointer_log_scores, 1)\n",
    "        # cat是在现有维度上续接，不会产生新维度\n",
    "        # t * (batch_size, 1) cat之后变成 (batch_size, max_seq_len)\n",
    "        pointer_argmaxs = torch.cat(pointer_argmaxs, 1)\n",
    "\n",
    "        return pointer_log_scores, pointer_argmaxs, mask_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def tsp_opt(points):\n",
    "    \"\"\"\n",
    "    Dynamic programing solution for TSP - O(2^n*n^2)\n",
    "    https://gist.github.com/mlalevic/6222750\n",
    "    :param points: List of (x, y) points\n",
    "    :return: Optimal solution\n",
    "    \"\"\"\n",
    "\n",
    "    def length(x_coord, y_coord):\n",
    "        return np.linalg.norm(np.asarray(x_coord) - np.asarray(y_coord))\n",
    "\n",
    "    # Calculate all lengths\n",
    "    all_distances = [[length(x, y) for y in points] for x in points]\n",
    "    # Initial value - just distance from 0 to every other point + keep the track of edges\n",
    "    a = {(frozenset([0, idx+1]), idx+1): (dist, [0, idx+1]) for idx, dist in enumerate(all_distances[0][1:])}\n",
    "    cnt = len(points)\n",
    "    for m in range(2, cnt):\n",
    "        b = {}\n",
    "        for S in [frozenset(C) | {0} for C in itertools.combinations(range(1, cnt), m)]:\n",
    "            for j in S - {0}:\n",
    "                # This will use 0th index of tuple for ordering, the same as if key=itemgetter(0) used\n",
    "                b[(S, j)] = min([(a[(S-{j}, k)][0] + all_distances[k][j], a[(S-{j}, k)][1] + [j])\n",
    "                                 for k in S if k != 0 and k != j])\n",
    "        a = b\n",
    "    res = min([(a[d][0] + all_distances[0][d[1]], a[d][1]) for d in iter(a)])\n",
    "    return np.asarray(res[1])\n",
    "\n",
    "\n",
    "class TSPDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Random TSP dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_size, min_seq_len, max_seq_len, solver=tsp_opt, solve=True):\n",
    "        self.data_size = data_size\n",
    "        self.min_leq_len = min_seq_len\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.solve = solve\n",
    "        self.solver = solver\n",
    "        self.data = self._generate_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor = torch.from_numpy(self.data['Points_List'][idx]).float()\n",
    "        length = len(self.data['Points_List'][idx])\n",
    "        solution = torch.from_numpy(self.data['Solutions'][idx]).long() if self.solve else None\n",
    "        return tensor, length, solution\n",
    "\n",
    "    def _generate_data(self):\n",
    "        \"\"\"\n",
    "        :return: Set of points_list ans their One-Hot vector solutions\n",
    "        \"\"\"\n",
    "        points_list = []\n",
    "        solutions = []\n",
    "        data_iter = tqdm(range(self.data_size), unit='data')\n",
    "        for i, _ in enumerate(data_iter):\n",
    "            data_iter.set_description('Data points %i/%i' % (i+1, self.data_size))\n",
    "            points_list.append(np.random.random((np.random.randint(self.min_leq_len, self.max_seq_len), 2)))\n",
    "        solutions_iter = tqdm(points_list, unit='solve')\n",
    "        if self.solve:\n",
    "            for i, points in enumerate(solutions_iter):\n",
    "                solutions_iter.set_description('Solved %i/%i' % (i+1, len(points_list)))\n",
    "                solutions.append(self.solver(points))\n",
    "        else:\n",
    "            solutions = None\n",
    "\n",
    "        return {'Points_List':points_list, 'Solutions':solutions}\n",
    "\n",
    "    def _to1hot_vec(self, points):\n",
    "        \"\"\"\n",
    "        :param points: List of integers representing the points indexes\n",
    "        :return: Matrix of One-Hot vectors\n",
    "        \"\"\"\n",
    "        vec = np.zeros((len(points), self.max_seq_len))\n",
    "        for i, v in enumerate(vec):\n",
    "            v[points[i]] = 1\n",
    "\n",
    "        return vec\n",
    "\n",
    "\n",
    "def sparse_seq_collate_fn(batch):\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    sorted_seqs, sorted_lengths, sorted_label = zip(*sorted(batch, key=lambda x:x[1], reverse=True))\n",
    "\n",
    "    padded_seqs = [seq.resize_as_(sorted_seqs[0]) for seq in sorted_seqs]\n",
    "\n",
    "    # (sparse) batch_size X max_seq_len X input_dim\n",
    "    seq_tensor = torch.stack(padded_seqs)\n",
    "\n",
    "    # batch_size\n",
    "    length_tensor = torch.LongTensor(sorted_lengths)\n",
    "\n",
    "    padded_labels = list(zip(*(itertools.zip_longest(*sorted_label, fillvalue=-1))))\n",
    "\n",
    "    # batch_size X max_seq_len (-1 padding)\n",
    "    label_tensor = torch.LongTensor(padded_labels).view(batch_size, -1)\n",
    "\n",
    "    # TODO: Currently, PyTorch DataLoader with num_workers >= 1 (multiprocessing) does not support Sparse Tensor\n",
    "    # TODO: Meanwhile, use a dense tensor when num_workers >= 1.\n",
    "    # seq_tensor = seq_tensor.to_dense()\n",
    "\n",
    "    return seq_tensor, length_tensor, label_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.__init__()\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def masked_accuracy(output, target, mask):\n",
    "    with torch.no_grad():\n",
    "        masked_output = torch.masked_select(output, mask)\n",
    "        masked_target = torch.masked_select(target, mask)\n",
    "        accuracy = masked_output.eq(masked_target).float().mean()\n",
    "\n",
    "        return accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载训练数据集\n",
      "加载训练数据集完成，开始加载测试数据集\n",
      "加载测试数据集完成\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "min_length = 5\n",
    "max_length = 10\n",
    "batch_size1 = 256\n",
    "no_cuda = False\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "cudnn.benchmark = True if use_cuda else False\n",
    "\n",
    "print(\"正在加载训练数据集\")\n",
    "# train_set = TSPDataset(data_size = 100000, min_seq_len=min_length, max_seq_len=max_length)\n",
    "\n",
    "filename = \"data/tsp_5_to_10_100000.pkl\"\n",
    "with open(filename, 'rb') as f:\n",
    "    train_set = pickle.load(f)   # read file and build object\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size1, shuffle=True, collate_fn=sparse_seq_collate_fn)\n",
    "print(\"加载训练数据集完成，开始加载测试数据集\")\n",
    "\n",
    "# test_set = TSPDataset(data_size = 10000, min_seq_len=min_length, max_seq_len=max_length)\n",
    "\n",
    "filename = \"data/tsp_5_to_10_10000.pkl\"\n",
    "with open(filename, 'rb') as f:\n",
    "    test_set = pickle.load(f)   # read file and build object\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size1, shuffle=False, collate_fn=sparse_seq_collate_fn)\n",
    "print(\"加载测试数据集完成\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PointerNet(\n",
      "  (embedding): Linear(in_features=2, out_features=100, bias=False)\n",
      "  (encoder): Encoder(\n",
      "    (rnn): LSTM(100, 100, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (decoding_rnn): LSTMCell(100, 100)\n",
      "  (attn): Attention(\n",
      "    (W1): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W2): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (vt): Linear(in_features=100, out_features=1, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 100\n",
    "epochs = 20\n",
    "\n",
    "# model = PointerNet(input_dim=2, embedding_dim=emb_dim, hidden_size=emb_dim).to(device)\n",
    "f = open(\"model/model_5_to_10_100000_epoch_100_acc_65.3.pkl\", 'rb')\n",
    "model = pickle.load(f)\n",
    "f.close()\n",
    "print(model)\n",
    "train_loss = AverageMeter()\n",
    "train_accuracy = AverageMeter()\n",
    "test_loss = AverageMeter()\n",
    "test_accuracy = AverageMeter()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "wd = 1e-5\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "def train():\n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for batch_idx, (seq, length, target) in enumerate(train_loader):\n",
    "            seq, length, target = seq.to(device), length.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            log_pointer_score, argmax_pointer, mask = model(seq, length)\n",
    "\n",
    "            # (batch * max_seq_len, max_seq_len)\n",
    "            unrolled = log_pointer_score.view(-1, log_pointer_score.size(-1))\n",
    "            # (batch_size, max_seq_len)\n",
    "            loss = F.nll_loss(unrolled, target.view(-1), ignore_index=-1)\n",
    "            assert not np.isnan(loss.item()), 'Model diverged with loss = NaN'\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.update(loss.item(), seq.size(0))\n",
    "\n",
    "            mask = mask[:, 0, :]\n",
    "            train_accuracy.update(masked_accuracy(argmax_pointer, target, mask).item(), mask.int().sum().item())\n",
    "\n",
    "            if batch_idx % 20 == 0:\n",
    "                print('Epoch {}: Train [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {:.6f}'\n",
    "                    .format(epoch, (batch_idx+1) * len(seq), len(train_loader.dataset),\n",
    "                        100. * (batch_idx+1) / len(train_loader), train_loss.avg, train_accuracy.avg))\n",
    "\n",
    "        # Test\n",
    "        model.eval()\n",
    "        for seq, length, target in test_loader:\n",
    "            seq, length, target = seq.to(device), length.to(device), target.to(device)\n",
    "            log_pointer_score, argmax_pointer, mask = model(seq, length)\n",
    "            unrolled = log_pointer_score.view(-1, log_pointer_score.size(-1))\n",
    "            loss = F.nll_loss(unrolled, target.view(-1), ignore_index=-1)\n",
    "            assert not np.isnan(loss.item()), 'Model diverged with loss = NaN'\n",
    "\n",
    "            test_loss.update(loss.item(), seq.size(0))\n",
    "\n",
    "            mask = mask[:, 0, :]\n",
    "            test_accuracy.update(masked_accuracy(argmax_pointer, target, mask).item(), mask.int().sum().item())\n",
    "        print('Epoch {}: Test\\tLoss: {:.6f}\\tAccuracy: {:.6f}'.format(epoch, test_loss.avg, test_accuracy.avg))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def get_real_and_predict_length(seqs, lengths, argmax_pointer, target):\n",
    "    with torch.no_grad():\n",
    "        # seqs = seqs.cpu().numpy()\n",
    "        # lengths = lengths.cpu().numpy()\n",
    "        # argmax_pointer = argmax_pointer.cpu().numpy()\n",
    "        # target = target.cpu().numpy()\n",
    "        predict_lengths = []\n",
    "        real_lengths = []\n",
    "\n",
    "        acc_count = 0\n",
    "        wrong_count = 0\n",
    "        real_path = []\n",
    "        wrong_path = []\n",
    "        for seq_index in range(len(seqs)):\n",
    "            cities = seqs[seq_index]\n",
    "            city_num = lengths[seq_index]\n",
    "            d = np.zeros((city_num, city_num))\n",
    "            predict_seq = list(argmax_pointer[seq_index, :city_num])\n",
    "            # print(\"预测路线：\", predict_seq)\n",
    "            real_seq = list(target[seq_index, :city_num])\n",
    "            # print(\"实际路线：\", real_seq)\n",
    "\n",
    "            # 计算完全预测正确的个数\n",
    "            if predict_seq == real_seq or list(reversed(predict_seq[1:])) == real_seq[1:]:\n",
    "                acc_count += 1\n",
    "                continue\n",
    "\n",
    "            # 计算预测错误，没有经过所有城市的个数\n",
    "            if len(set(predict_seq)) != len(predict_seq):\n",
    "                wrong_count += 1\n",
    "                wrong_path.append(predict_seq)\n",
    "                real_path.append(real_seq)\n",
    "                continue\n",
    "\n",
    "            for i in range(city_num):\n",
    "                for j in range(i, city_num):\n",
    "                    d[j][i] = d[i][j] = ((cities[i][0] - cities[j][0]) ** 2 + (cities[i][1] - cities[j][1]) ** 2) ** 0.5\n",
    "            predict_length = 0\n",
    "            for i in range(city_num - 1):\n",
    "                predict_length += d[predict_seq[i]][predict_seq[i+1]]\n",
    "            predict_length += d[predict_seq[-1]][predict_seq[0]]\n",
    "            # print(\"预测距离：\", predict_length)\n",
    "            predict_lengths.append(predict_length)\n",
    "\n",
    "            real_length = 0\n",
    "            for i in range(city_num - 1):\n",
    "                real_length += d[real_seq[i]][real_seq[i+1]]\n",
    "            real_length += d[real_seq[-1]][real_seq[0]]\n",
    "            # print(\"实际距离：\", real_length)\n",
    "            real_lengths.append(real_length)\n",
    "        predict_mean = np.mean(predict_lengths)\n",
    "        real_mean = np.mean(real_lengths)\n",
    "        print(\"完全预测正确的个数：{}，比例为 {}%\".format(acc_count, 100.0 * acc_count / len(seqs)))\n",
    "        if wrong_count != 0:\n",
    "            print(\"出现重复城市的个数：{}，比例为 {}%\".format(wrong_count, 100.0 * wrong_count / len(seqs)))\n",
    "            print(\"错误情况如下：\")\n",
    "            for i in range(wrong_count):\n",
    "                print(\"\\t实际路径为：\", real_path[i])\n",
    "                print(\"\\t预测路径为：\", wrong_path[i])\n",
    "        print(\"预测不完全相同的情况如下：\")\n",
    "        print(\"平均预测距离：\", predict_mean)\n",
    "        print(\"平均实际距离：\", real_mean)\n",
    "        print(\"预测长度比例：\", predict_mean / real_mean)\n",
    "        print()\n",
    "\n",
    "\n",
    "def evaluate(use_test_loader):\n",
    "    model.eval()\n",
    "    for idx, (seq, length, target) in enumerate(use_test_loader):\n",
    "        seq, length, target = seq.to(device), length.to(device), target.to(device)\n",
    "        log_pointer_scores, argmax_pointers, mask = model(seq, length)\n",
    "        print(\"Batch {}\".format(idx), \"-------------------------------\")\n",
    "        get_real_and_predict_length(seq, length, argmax_pointers, target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ccjjx\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:584: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ..\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:775.)\n",
      "  result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 -------------------------------\n",
      "完全预测正确的个数：106，比例为 41.40625%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.7567733394044143\n",
      "平均实际距离： 2.5702851233196755\n",
      "预测长度比例： 1.0725554586892982\n",
      "\n",
      "Batch 1 -------------------------------\n",
      "完全预测正确的个数：106，比例为 41.40625%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.6872243474175534\n",
      "平均实际距离： 2.5293241013462344\n",
      "预测长度比例： 1.0624278422790012\n",
      "\n",
      "Batch 2 -------------------------------\n",
      "完全预测正确的个数：107，比例为 41.796875%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.65945846683677\n",
      "平均实际距离： 2.460891201701515\n",
      "预测长度比例： 1.0806891686223108\n",
      "\n",
      "Batch 3 -------------------------------\n",
      "完全预测正确的个数：101，比例为 39.453125%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.6263213832412036\n",
      "平均实际距离： 2.4795446080185712\n",
      "预测长度比例： 1.0591950532964693\n",
      "\n",
      "Batch 4 -------------------------------\n",
      "完全预测正确的个数：111，比例为 43.359375%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.830000147061294\n",
      "平均实际距离： 2.5768101949614057\n",
      "预测长度比例： 1.0982571213801335\n",
      "\n",
      "Batch 5 -------------------------------\n",
      "完全预测正确的个数：110，比例为 42.96875%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.7590561867691576\n",
      "平均实际距离： 2.5563875087792027\n",
      "预测长度比例： 1.0792793257258322\n",
      "\n",
      "Batch 6 -------------------------------\n",
      "完全预测正确的个数：122，比例为 47.65625%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.7455936230427183\n",
      "平均实际距离： 2.5356198267232792\n",
      "预测长度比例： 1.0828096523408177\n",
      "\n",
      "Batch 7 -------------------------------\n",
      "完全预测正确的个数：103，比例为 40.234375%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.819150721648209\n",
      "平均实际距离： 2.5546981871335044\n",
      "预测长度比例： 1.1035161553903292\n",
      "\n",
      "Batch 8 -------------------------------\n",
      "完全预测正确的个数：101，比例为 39.453125%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.7339618214195776\n",
      "平均实际距离： 2.4945561645792855\n",
      "预测长度比例： 1.0959712433977884\n",
      "\n",
      "Batch 9 -------------------------------\n",
      "完全预测正确的个数：109，比例为 42.578125%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.7920669133451823\n",
      "平均实际距离： 2.5264342322590805\n",
      "预测长度比例： 1.1051413401917765\n",
      "\n",
      "Batch 10 -------------------------------\n",
      "完全预测正确的个数：97，比例为 37.890625%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.651630662381649\n",
      "平均实际距离： 2.491137838087765\n",
      "预测长度比例： 1.0644255094359132\n",
      "\n",
      "Batch 11 -------------------------------\n",
      "完全预测正确的个数：107，比例为 41.796875%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.732217119631651\n",
      "平均实际距离： 2.547986535192936\n",
      "预测长度比例： 1.0723043791221467\n",
      "\n",
      "Batch 12 -------------------------------\n",
      "完全预测正确的个数：114，比例为 44.53125%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.62537222917677\n",
      "平均实际距离： 2.5389489808552703\n",
      "预测长度比例： 1.0340389858060035\n",
      "\n",
      "Batch 13 -------------------------------\n",
      "完全预测正确的个数：98，比例为 38.28125%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.7772590065643166\n",
      "平均实际距离： 2.5913662290704567\n",
      "预测长度比例： 1.0717354326102109\n",
      "\n",
      "Batch 14 -------------------------------\n",
      "完全预测正确的个数：102，比例为 39.84375%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.673525069616493\n",
      "平均实际距离： 2.5164141663767445\n",
      "预测长度比例： 1.062434437597355\n",
      "\n",
      "Batch 15 -------------------------------\n",
      "完全预测正确的个数：105，比例为 41.015625%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.7513970140959847\n",
      "平均实际距离： 2.4954154415053025\n",
      "预测长度比例： 1.1025807440048008\n",
      "\n",
      "Batch 16 -------------------------------\n",
      "完全预测正确的个数：120，比例为 46.875%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.7155768055338685\n",
      "平均实际距离： 2.4893790975055547\n",
      "预测长度比例： 1.0908651110049778\n",
      "\n",
      "Batch 17 -------------------------------\n",
      "完全预测正确的个数：94，比例为 36.71875%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.7726184543981045\n",
      "平均实际距离： 2.5194083289530727\n",
      "预测长度比例： 1.1005038058083472\n",
      "\n",
      "Batch 18 -------------------------------\n",
      "完全预测正确的个数：105，比例为 41.015625%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.7360450213078087\n",
      "平均实际距离： 2.5374888364034427\n",
      "预测长度比例： 1.0782490870721597\n",
      "\n",
      "Batch 19 -------------------------------\n",
      "完全预测正确的个数：110，比例为 42.96875%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.7515552057693267\n",
      "平均实际距离： 2.5780990432105857\n",
      "预测长度比例： 1.0672806434708306\n",
      "\n",
      "Batch 20 -------------------------------\n",
      "完全预测正确的个数：107，比例为 41.796875%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.718407900091386\n",
      "平均实际距离： 2.5471608051030126\n",
      "预测长度比例： 1.0672305787075926\n",
      "\n",
      "Batch 21 -------------------------------\n",
      "完全预测正确的个数：108，比例为 42.1875%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.6731263314518525\n",
      "平均实际距离： 2.459840074360899\n",
      "预测长度比例： 1.0867073674073582\n",
      "\n",
      "Batch 22 -------------------------------\n",
      "完全预测正确的个数：107，比例为 41.796875%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.8328003181744643\n",
      "平均实际距离： 2.6078278959084975\n",
      "预测长度比例： 1.0862681247558297\n",
      "\n",
      "Batch 23 -------------------------------\n",
      "完全预测正确的个数：117，比例为 45.703125%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.698584428378682\n",
      "平均实际距离： 2.5358449413303767\n",
      "预测长度比例： 1.0641756459142677\n",
      "\n",
      "Batch 24 -------------------------------\n",
      "完全预测正确的个数：105，比例为 41.015625%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.741275650961016\n",
      "平均实际距离： 2.589604172911125\n",
      "预测长度比例： 1.0585693673328416\n",
      "\n",
      "Batch 25 -------------------------------\n",
      "完全预测正确的个数：97，比例为 37.890625%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.717862555429152\n",
      "平均实际距离： 2.5059285657890373\n",
      "预测长度比例： 1.0845730371302038\n",
      "\n",
      "Batch 26 -------------------------------\n",
      "完全预测正确的个数：107，比例为 41.796875%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.7594449538737535\n",
      "平均实际距离： 2.575688553583377\n",
      "预测长度比例： 1.0713426318701185\n",
      "\n",
      "Batch 27 -------------------------------\n",
      "完全预测正确的个数：113，比例为 44.140625%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.8228407461339464\n",
      "平均实际距离： 2.57676953010235\n",
      "预测长度比例： 1.0954960128008897\n",
      "\n",
      "Batch 28 -------------------------------\n",
      "完全预测正确的个数：112，比例为 43.75%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.7297835056087933\n",
      "平均实际距离： 2.5196709286667303\n",
      "预测长度比例： 1.0833888959671583\n",
      "\n",
      "Batch 29 -------------------------------\n",
      "完全预测正确的个数：111，比例为 43.359375%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.6299691886223595\n",
      "平均实际距离： 2.4754557967828266\n",
      "预测长度比例： 1.0624181583207193\n",
      "\n",
      "Batch 30 -------------------------------\n",
      "完全预测正确的个数：104，比例为 40.625%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.771329118488785\n",
      "平均实际距离： 2.525756198451793\n",
      "预测长度比例： 1.0972274838670177\n",
      "\n",
      "Batch 31 -------------------------------\n",
      "完全预测正确的个数：108，比例为 42.1875%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.7342453136964626\n",
      "平均实际距离： 2.5152553351079994\n",
      "预测长度比例： 1.087064710899047\n",
      "\n",
      "Batch 32 -------------------------------\n",
      "完全预测正确的个数：98，比例为 38.28125%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.6168812335574927\n",
      "平均实际距离： 2.452798368067398\n",
      "预测长度比例： 1.0668961899299445\n",
      "\n",
      "Batch 33 -------------------------------\n",
      "完全预测正确的个数：114，比例为 44.53125%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.6345884322818423\n",
      "平均实际距离： 2.460642020460743\n",
      "预测长度比例： 1.0706914741659694\n",
      "\n",
      "Batch 34 -------------------------------\n",
      "完全预测正确的个数：113，比例为 44.140625%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.6423686229757366\n",
      "平均实际距离： 2.473765395900847\n",
      "预测长度比例： 1.0681565144998284\n",
      "\n",
      "Batch 35 -------------------------------\n",
      "完全预测正确的个数：98，比例为 38.28125%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.7027247612293857\n",
      "平均实际距离： 2.470187513272667\n",
      "预测长度比例： 1.0941374882300485\n",
      "\n",
      "Batch 36 -------------------------------\n",
      "完全预测正确的个数：101，比例为 39.453125%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.7154064607115522\n",
      "平均实际距离： 2.5077181888053253\n",
      "预测长度比例： 1.0828196217714436\n",
      "\n",
      "Batch 37 -------------------------------\n",
      "完全预测正确的个数：109，比例为 42.578125%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.748591058973071\n",
      "平均实际距离： 2.5236601175037117\n",
      "预测长度比例： 1.089128856896883\n",
      "\n",
      "Batch 38 -------------------------------\n",
      "完全预测正确的个数：99，比例为 38.671875%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.7172160967176033\n",
      "平均实际距离： 2.5367154478611553\n",
      "预测长度比例： 1.0711552606377819\n",
      "\n",
      "Batch 39 -------------------------------\n",
      "完全预测正确的个数：3，比例为 18.75%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 2.5346640003128695\n",
      "平均实际距离： 2.2869331947026343\n",
      "预测长度比例： 1.108324460978602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data2 = TSPDataset(data_size=1000, min_seq_len=10, max_seq_len=15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 -------------------------------\n",
      "完全预测正确的个数：1，比例为 1.0%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 3.6189988685038053\n",
      "平均实际距离： 3.1116949602246584\n",
      "预测长度比例： 1.163031375107064\n",
      "\n",
      "Batch 1 -------------------------------\n",
      "完全预测正确的个数：0，比例为 0.0%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 3.544474756848067\n",
      "平均实际距离： 3.0858825590647756\n",
      "预测长度比例： 1.1486097377348912\n",
      "\n",
      "Batch 2 -------------------------------\n",
      "完全预测正确的个数：2，比例为 2.0%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 3.3976604527964884\n",
      "平均实际距离： 3.070552528847712\n",
      "预测长度比例： 1.1065306393151106\n",
      "\n",
      "Batch 3 -------------------------------\n",
      "完全预测正确的个数：3，比例为 3.0%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 3.562651682222627\n",
      "平均实际距离： 3.065534104842731\n",
      "预测长度比例： 1.1621634470138766\n",
      "\n",
      "Batch 4 -------------------------------\n",
      "完全预测正确的个数：1，比例为 1.0%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 3.436096372905968\n",
      "平均实际距离： 3.0895305540026965\n",
      "预测长度比例： 1.1121742649394653\n",
      "\n",
      "Batch 5 -------------------------------\n",
      "完全预测正确的个数：1，比例为 1.0%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 3.4210910900223133\n",
      "平均实际距离： 3.092931026254188\n",
      "预测长度比例： 1.106100026474097\n",
      "\n",
      "Batch 6 -------------------------------\n",
      "完全预测正确的个数：0，比例为 0.0%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 3.6470484534744174\n",
      "平均实际距离： 3.1094263430684803\n",
      "预测长度比例： 1.1729007382999768\n",
      "\n",
      "Batch 7 -------------------------------\n",
      "完全预测正确的个数：0，比例为 0.0%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 3.5095987439993768\n",
      "平均实际距离： 3.107461981708184\n",
      "预测长度比例： 1.1294100345099434\n",
      "\n",
      "Batch 8 -------------------------------\n",
      "完全预测正确的个数：1，比例为 1.0%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 3.6368226317895784\n",
      "平均实际距离： 3.1042376078237224\n",
      "预测长度比例： 1.1715670935187315\n",
      "\n",
      "Batch 9 -------------------------------\n",
      "完全预测正确的个数：1，比例为 1.0%\n",
      "预测不完全相同的情况如下：\n",
      "平均预测距离： 3.55696407912506\n",
      "平均实际距离： 3.0697704796124285\n",
      "预测长度比例： 1.1587068488501922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader2 = DataLoader(dataset=test_data2, batch_size=100, shuffle=False, collate_fn=sparse_seq_collate_fn)\n",
    "evaluate(test_loader2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-9026f3d8",
   "language": "python",
   "display_name": "PyCharm (Ptr-Net)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}